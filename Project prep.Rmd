---
title: "Project 3"
author: "Kristin Calvert"
date: "7/15/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(shiny)
library(tidyverse)
library(knitr)
library(DT)
library(caret)
library(randomForest)
library(leaflet)
```

## Project 3 

About the Survey

from 
[https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey](About the PLS)
[https://www.imls.gov/sites/default/files/fy2014_pls_data_file_documentation.pdf](Documentation)

The Public Libraries Survey (PLS) examines when, where, and how library services are changing to meet the needs of the public. These data, supplied annually by public libraries across the country, provide information that policymakers and practitioners can use to make informed decisions about the support and strategic management of libraries.

Purpose: The survey provides statistics on the status of public libraries in the United States.

Coverage: The data are collected from approximately 9,000 public libraries with approximately 17,000 individual public library outlets (main libraries, branches, and bookmobiles) in the 50 states, the District of Columbia, and outlying territories.

Content: Data includes information about library visits, circulation, size of collections, public service hours, staffing, electronic resources, operating revenues and expenditures and number of service outlets. Learn more about PLS data element definitions.

Frequency: Collected annually since 1988. (Data files are available since 1992.)

Methods: At the state level, PLS is administered by Data Coordinators, appointed by the chief officer of the state library agency from each state or outlying area. State Data Coordinators collect the requested data from local public libraries and report these data to us via a web-based reporting system.

Use: PLS data are useful to researchers, journalists, the public, local practitioners, and policymakers at the federal, state, and local levels, and are used for planning, evaluation, and policy making. Download the datasets in multiple formats below, or use our online Library Search & Compare tool to find a library and browse the latest available data. Browse over 25 yearsâ€™ worth of research publications about the Public Libraries Survey (PLS). 

```{r data_ingest}

#read in data
data_by_library <- read_csv("PLS/libraries.csv")

data_by_state <- read_csv("PLS/states.csv")

#Fix column names
colnames(data_by_state) <- make.names(colnames(data_by_state))
colnames(data_by_library) <- make.names(colnames(data_by_library))

#convert columns to factors
data_by_state$State <- as.factor(data_by_state$State)
data_by_state$Region.Code <- as.factor(data_by_state$Region.Code)

#create a new column for total number of libraries
data_by_state$Total.Libraries <- data_by_state$Central.Libraries+data_by_state$Branch.Libraries

#subset data for southeast region
data_by_region_se <- data_by_state %>% filter(Region.Code == "5")
```

## Data Exploration

```{r data_summary}
#Summarize by region
regional_summary <- data_by_state %>% group_by(Region.Code) %>% summarise("Average Service Population" = mean(Service.Population), "Average Service Population per Library" = mean(Service.Population)/(sum(Central.Libraries)+sum(Branch.Libraries)), "Average Library Employees per 10,000 People" = mean(Employees)/sum(Service.Population)*10000, "Average Collections Spending per 10,000 People" = mean(Total.Collection.Expenditures)/sum(Service.Population)*10000, "Total Visits" = sum(Library.Visits))


#Create groupings of columns
Staffing <- c("MLS.Librarians", "Librarians", "Employees", "Total.Staff")
Finances <- c("Total.Staff.Expenditures", "Total.Operating.Expenditures", "Print.Collection.Expenditures",
"Digital.Collection.Expenditures", "Other.Collection.Expenditures", "Total.Collection.Expenditures")
Collections <- c("Print.Collection", "Digital.Collection", "Audio.Collection", "Downloadable.Audio", "Physical.Video", "Downloadable.Video", "State.Licensed.Databases", "Total.Licensed.Databases", "Print.Subscriptions")

#Function to create a summary table based on grouped columns
subset_data <- function(x){
  subset <- data_by_state %>% select(x)
  kable(round(apply(subset, 2, summary), 2), caption = paste("State-Level Summary Statistics for", deparse(substitute(x))))
}

subset_data(Staffing)
subset_data(Finances)
subset_data(Collections)

#Scatter plot

g <- ggplot(data = data_by_state, aes(x = Circulation.Transactions, y = Service.Population, group = Region.Code))
g + geom_point(aes(color = Library.Visits))

gp <- ggplot(data = data_by_state, aes(x = Total.Staff.Expenditures, y = Circulation.Transactions))
gp + geom_point(aes(color = Region.Code))



```

```{r box_and_bar_plots}
#Box plot function for operating costs by region

##plot_costs_by_region <- function(region, cost){
  
    #Switch from string to code for region
#  region <- switch(input$region, "New England (CT ME MA NH RI VT)" = "1" , "Mid East (DE DC MD NH NY PA)" = "2", "Great Lakes(IL IN MI OH WI" = "3", "Plains (IA KS MN MO NE ND SD" ="4", "Southeast (AL AR FL GA KY L MS NC SC TN VA WV" = "5", "Southwest (AZ NM OK TX)" = "6", "Rocky Mountains (CO ID MT UT WY)" = "7", "Far West (AK CA HI NV OR WA)" = "8")  ##Switch to dynamic input$region
  
    #Function to subset data by selected region
  subset_by_region <- function(region){
  data_by_region <- data_by_state %>% filter(Region.Code == region) 
  }
  
    #Bar chart of selected cost variable by selected region
  ggplot(data = data_by_region_se, aes(x = State, y = Total.Operating.Expenditures/State.Population)) + ##switch to input$cost_type and input$region
    geom_bar(stat = "Identity") + 
    ggtitle("Per Capita Public Library Operating Costs for Southeast Region (US).") + ##Switch to dynamic title
    ylab("Dollars Spent per Capita")
##}
```

```{r PCA}

#Create the PCs. Not to be adjustable by input
PCs <- prcomp(select(data_by_state, Service.Population, Total.Libraries, Total.Staff, Local.Government.Operating.Revenue, Total.Operating.Revenue, Total.Staff.Expenditures, Print.Collection.Expenditures, Total.Collection.Expenditures, Total.Operating.Expenditures, Print.Collection, Audio.Collection, Physical.Video, Print.Subscriptions, Hours.Open, Library.Visits, Circulation.Transactions, Library.Programs, Library.Programs, Public.Internet.Computers, Internet.Computer.Use), center = TRUE, scale = TRUE)

#Create the scree plot
screeplot(PCs, type = "lines")

#Create plots to show proportion of variance and cummulative variance by PC
par(mfrow = c(1, 2))
plot(PCs$sdev^2/sum(PCs$sdev^2), xlab = "Principal Component", 
         ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(cumsum(PCs$sdev^2/sum(PCs$sdev^2)), xlab = "Principal Component", 
ylab = "Cum. Prop of Variance Explained", ylim = c(0, 1), type = 'b')

#Create Biplot
##Must add user input to view PCs
biplot(PCs, xlabs = rep(".", nrow(data_by_state)), choices = c(2,3), cex = .6)

autoplot(PCs, x =2, y= 3, scale = 0, loadings = TRUE, loadings.label = TRUE, loadings.label.size = 3)

##Must add dynamic table print for PCs selected in the biplot
kable(PCs$rotation[,c("PC1","PC2")])

```

```{r modeling}

#Bagged tree model to predict Salary expenditure

#Exclude columns with no variance

state_data_limited <- data_by_state %>% dplyr::select_if(is.numeric) %>% dplyr::select(-Submission.Year, -State.Code, -Region.Code)
state_data_limited <- data_by_state %>% dplyr::select(-Submission.Year, -State.Code, -Region.Code, -Service.Population.Without.Duplicates) %>% dplyr::select_if(is.numeric) 

train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
tree_fit <- train(Library.Visits ~ ., data = state_data_train,
                  method = "rpart2",
                  trControl = train_control,
                  tuneGrid = grid)

grid <- expand.grid(maxdepth = 5)

set.seed(input$seed)
train <- sample(1:nrow(state_data_limited), size = nrow(state_data_limited)*.8)
test <- dplyr::setdiff(1:nrow(state_data_limited), train)

state_data_train <- state_data_limited[train,]
state_data_test <- state_data_limited[test,]

#Create training and test sets
set.seed(250)
train <- sample(1:nrow(state_data_limited), size = nrow(state_data_limited)*.8)
test <- dplyr::setdiff(1:nrow(state_data_limited), train)

state_data_train <- state_data_limited[train,]
state_data_test <- state_data_limited[test,]

#Fit bagged model
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

bag_tree_fit <- train(Salaries ~ .,
                data = state_data_train,
                method = "treebag",
                trControl = train_control)

bag_predict <- predict(bag_tree_fit, newdata = dplyr::select(state_data_test, -Salaries))
bag_RMSE <- sqrt(mean((bag_predict-state_data_test$Salaries)^2))


#Simple tree model to predict salary expenditure
tree_fit <- tree(Salaries ~ ., data = state_data_limited, split = "deviance", control = tree.control(nobs = nrow(state_data_limited), mincut = 3))

plot(tree_fit)
text(tree_fit, cex=.8)
cv_tree <- cv.tree(tree_fit); cv_tree
plot(cv_tree$size, cv_tree$dev, type ="b")

tree_predict <- predict(tree_fit, newdata = dplyr::select(state_data_test, -Salaries))
tree_RMSE <- sqrt(mean((tree_predict-state_data_test$Salaries)^2))

predict(tree_fit, newdata = data.frame(Region.Code = "5", Service.Population = 2000000, Circulation.Transactions = 1000000, Library.Visits = 800000, Library.Programs = 8800))


fancyRpartPlot(tree_fit$finalModel)


```

```{r maps, message = FALSE}

map_data <- data_by_library %>% dplyr::select(Library.Name, State, Zip.Code, latitude = Latitude, longitude = Longitude, County.Population)

leaflet(data = map_data) %>% 
  addTiles %>% 
  setView(zoom = 7, lng = -78.6821, lat = 35.7847) %>% 
  addCircleMarkers(clusterOptions = markerClusterOptions(),radius = ~ifelse(County.Population > 500000, 15, 10), color = "green", stroke = FALSE, fillOpacity = .5, popup = paste("<h4>", map_data$Library.Name,"</h4>", "<br>", "County Pop. = ", map_data$County.Population))
```

```{r}
state_data_limited <- data_by_state %>% dplyr::select(-Submission.Year, -State.Code,-Service.Population.Without.Duplicates)
state_data_limited$Region.Code <- as.factor(state_data_limited$Region.Code)
set.seed(250)
train <- sample(1:nrow(state_data_limited), size = nrow(state_data_limited)*.8)
test <- dplyr::setdiff(1:nrow(state_data_limited), train)

state_data_train <- state_data_limited[train,]
state_data_test <- state_data_limited[test,]

class_tree_fit <- tree(Region.Code ~ Library.Programs + Print.Collection + Hours.Open + Total.Libraries + Library.Visits, data = state_data_train, split = "deviance")
class_tree_fit
plot(class_tree_fit)
text(class_tree_fit)

summary(class_tree_fit)

predict(class_tree_fit, newdata = dplyr::select(state_data_limited, Library.Programs, Print.Collection, Hours.Open, Total.Libraries, Library.Visits))

str(data_by_state)







      grid <- expand.grid(maxdepth = 3)
      
      group_vars <- c("Library.Programs", "Print.Collection", "Hours.Open", "Total.Libraries", "Library.Visits")
      y <- data_by_state["Service.Population"]

      tree_fit <- train(as.formula(paste(y, paste0(group_vars, collapse="+"), sep=" ~ ")),
                        data = data_by_state,
                        method = "rpart2",
                        tuneGrid = grid)
      
      pred <- predict(tree_fit, newdata = dplyr::select(state_data_test, Library.Programs, Print.Collection, Hours.Open, Total.Libraries, Library.Visits))
      print(as.formula(paste(pred, y, sep = "-")))
            rmse <- sqrt(mean((as.formula(paste(pred, y,sep = "-"))^2)))
            
            
            eval(parse(text=paste(pred, y, sep = "-")))
```

